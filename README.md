# E_Commerce Order Tracking Pipeline

A real-time data pipeline using Apache Kafka, Apache Spark, and My SQL foe tracking e-commerce orders.

## ğŸš€ Teck Stack 
- Apache Kafka
- Apache Spark Structured Streaming
- MySQL
- Python

## ğŸ“¦ Project Overview
This project simulates a real-time e-commerce environment where order data is:
1. **Produced** via Kafka.
2. **Consumed and processed** by Spark.
3. **Stored** in MySQL for persistent analytics or dashboarding.

## ğŸ“Œ Workflow
1. ğŸ§¾ Kafka Producer sends JSON messages representing order events.
2. ğŸ”„ Spark Structured Streaming consumes the Kafka topic in micro-batches.
3. ğŸ›¢ï¸ Parsed data is written and appended to a MySQL table.

---

## ğŸ”§ Sample Input Message
json
{
  "user_id": 11111,
  "order_id": "ORD002",
  "item_name": "Shoes",
  "category": "Footwear",
  "price": 1499,
  "timestamp": "2025-06-25 14:00:00"
} ```

ğŸ—ƒï¸ MySQL Table Structure
Column Name                Data type 
user_id                    INT
order_id                   VARCHAR
item_name                  VARCHAR
category                   VARCHAR
price                      INT
timestamp                  DATETIME

ğŸ’¡ Features
	â€¢	Real-time stream processing using Spark.
	â€¢	JSON parsing and schema enforcement.
	â€¢	Batch-wise writing to MySQL.
	â€¢	Easy to extend with Apache Airflow and dashboards.

 ## ğŸ“ Project Files

- `kafka_order_producer.py` â€“ Sends sample order messages to Kafka topic `orders`.
- `spark_to_mysql.py` â€“ Reads Kafka stream and stores processed data in MySQL.
- `spark_order_consumer.py` â€“ (Optional) Displays Kafka messages using Spark, for testing/debugging.

ğŸ™‹ğŸ» About Me

Iâ€™m a final-year B.Tech student in Data Science ğŸ“
This project reflects my hands-on learning in real-time data engineering and stream processing.
Actively exploring opportunities to learn and grow in data infrastructure and big data pipelines ğŸš€

ğŸ”— Connect with me : 
â€¢ ğŸ’¼ http://linkedin.com/in/preetambaloda






